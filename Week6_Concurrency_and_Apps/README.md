# Section 3: Week 6: Concurrent Processing

The Merriam-Webster Dictionary defines concurrency as the fact of two or more events or circumstances happening or existing at the same time. In computer systems networks, concurrency—or concurrent processing—can be thought of as having multiple processes being executed simultaneously for performance improvement and network stability. To avoid processing delays and increase speed while executing tasks or processes, the process, message, or task is broken down into small packets or subtasks, distributed in multiple processors across the network, and executed at the same time—giving the impression that it’s only one process or task being executed.

Sometimes, concurrency issues are imposed by the demands of the environment in which they operate. Real-time systems present unique issues where events are occurring constantly in a simultaneous fashion and must be handled within a restricted timeframe. External events must be responded to, and they can occur at any time in any order. Conventional procedural programming languages result in rather complex solutions, so the systems are separated into concurrent units that handle each event as it occurs. The hard part is determining when this solution is the best fit since the complexity of the program will be affected by the number of interacting processes.

Another set of concurrent issues exists where the concurrency is created in the system on purpose. This is because performing tasks in parallel can substantially speed up the computational work of a system if multiple CPUs are available. Problems of single processor systems can be very similar to those faced by distributed systems. Multitasking on a single computer can greatly speed up multiple processes by utilizing the processor while it is doing I/O or a computation to complete. When a system is booted, it has to load many processes as quickly as possible; doing them sequentially would only increase the time to complete the system becoming ready.

Although achieving concurrency is easy with multiple processors and processes, the interactions become more complex. Since the tasks are running on different processors, how will they communicate? When determining how they communicate, how many layers of software must be involved and how will it affect overhead in the system? For instance, the CISC-based processors would appear to be better processors in theory; but in reality, they could be much slower than RISC processors due to concurrency issues.

Processes are an important concept in any system because they can have a large effect on the performance of the system. Pipelining can increase the throughput of instructions because multiple instructions are divided into stages to overlap in execution, depending on the number of times the times the instruction is executed. The goal is to balance execution speed and processor machine cycles.

This week, you will study how processors and processes are organized for allocation and scheduling for optimal execution and smooth network performance. You will also explore how concurrent processes have been handled in the past, and then examine how they may be handled in the future as processing power increases and network connections get faster.

The subject of how the processors and processes are organized will be covered and several different models presented, including how processor allocation and scheduling in distributed systems occurs. You will focus on the issues specific to multi-processors systems like you will find in distributed systems and how they handle multiple threads of control within a process.
